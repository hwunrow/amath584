# -*- coding: utf-8 -*-
"""hwunrow_amath584_hw6.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1okPbSGqe5fgb_keR9biC92qE0xrbCd3r
"""

# Commented out IPython magic to ensure Python compatibility.
import numpy as np
import pandas as pd
import tensorflow as tf

import matplotlib.pyplot as plt
# %matplotlib inline

from tensorflow.keras.datasets.mnist import load_data
from tensorflow.keras.utils import to_categorical

from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import GridSearchCV
from sklearn.metrics import confusion_matrix
from sklearn.metrics import plot_confusion_matrix
from sklearn.metrics import accuracy_score
from sklearn.metrics import classification_report

from google.colab import drive
drive.mount('/content/drive')

"""# Data Processing and EDA"""

data_dir = "/content/drive/MyDrive/School/AMATH584/repos/amath584/hw6/data/mnist.npz"
(x_train, y_train), (x_test, y_test) = load_data(path=data_dir)

print(f"training set dimensions: {x_train.shape}")
print(f"test set dimensions: {x_test.shape}")

x_train = x_train.reshape(60000, 784)
x_test = x_test.reshape(10000, 784)

# one-hot encode
y_train_encode = to_categorical(y_train, 10)
y_test_encode = to_categorical(y_test, 10)

"""Plot the first 10 numbers in training set"""

fig=plt.figure(figsize=(20,10))

rows = 2
columns = 5

for i in range(10):
  ax = fig.add_subplot(rows, columns, i+1)
  plt.imshow(x_train[i].reshape(28,28))
  plt.axis('off')

x_train.max()

# normalize
x_train = x_train/255
x_test = x_test/255

np.histogram(y_test, bins=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9])

fig=plt.figure(figsize=(20,10))

plt.hist(y_train, bins=[0,1,2,3,4,5,6,7,8,9, 10], ec='black')

fig=plt.figure(figsize=(20,10))

plt.hist(y_test, bins=[0,1,2,3,4,5,6,7,8,9,10], ec='black')

"""# Logistic Regression with LASSO, Ridge, and Elastic Net

## Hyperparameter Tuning
We use Grid Search to tune the type of regularization (L1 norm or Elastic Net) and size of the penalty.
"""

lr = LogisticRegression(tol=0.1)

param_grid = [
  {'C': [10**x for x in range(-5,5)], 'penalty': ['l1'], 'solver': ['saga']},
  {'C': [10**x for x in range(-5,5)], 'l1_ratio':[0.1*(x+1) for x in range(9)], 'penalty': ['elasticnet'], 'solver': ['saga']},
  ]

clf = GridSearchCV(lr, param_grid, cv=3, scoring='accuracy', n_jobs=-1)

clf.fit(x_train, y_train)

# import pickle
# filename = "/content/drive/MyDrive/School/AMATH584/repos/amath584/hw6/lr_norm_model.sav"
# pickle.dump(clf, open(filename, 'wb'))

# clf = pickle.load(open(filename, 'rb'))

"""### LASSO"""

lasso_accuracy = clf.cv_results_['mean_test_score'][0:10]

lasso_accuracy

plt.figure(figsize=(20,10))
#flip because the hyperparameter in scikit learn C = 1/lambda
plt.plot(list(range(-5,5)), np.flip(lasso_accuracy), '--o')  
plt.xlabel(r"$\lambda_1$ log-Scale",fontsize=22)
plt.ylabel("Mean 3-fold cross validation accuracy",fontsize=22)

plt.xticks(fontsize=20)
plt.yticks(fontsize=20)

lasso_best_params = clf.cv_results_['params'][np.argmax(lasso_accuracy)]
clf_lasso = LogisticRegression(**lasso_best_params)

clf_lasso.fit(x_train, y_train)

lasso_best_params

lasso_coefs = clf_lasso.coef_

print(f"{np.mean(lasso_coefs == 0) * 100:.2f} % of features are zero!")

plt.figure(figsize=(20,10))
plt.hist(lasso_coefs.flatten())

plt.xticks(fontsize=20)
plt.yticks(fontsize=20)

fig, axes = plt.subplots(nrows=2, ncols=5, figsize=(20,10))

scale = np.abs(lasso_coefs).max()

for i, ax in enumerate(axes.flat):
  ax.set_axis_off()
  ax.set_title(f"{i}")
  img = lasso_coefs[i]
  img = img.reshape(28,28)
  im = ax.imshow(img, interpolation='nearest', vmin=-scale, vmax=scale, cmap=plt.cm.RdBu)

cbar = fig.colorbar(im, ax=axes.ravel().tolist())

"""### Elastic Net"""

elastic_net_accuracy = clf.cv_results_['mean_test_score'][10:101]
elastic_net_params = clf.cv_results_['params'][10:101]

# rows represent same C, columns are for same l1_ratio
elastic_net_accuracy = elastic_net_accuracy.reshape(9,10)
#flip because the hyperparameter in scikit learn C = 1/lambda
elastic_net_accuracy = np.flip(elastic_net_accuracy, axis=1)

plt.figure(figsize=(20,10))
#flip because the hyperparameter in scikit learn C = 1/lambda
plt.plot(list(range(-5,5)), elastic_net_accuracy.T, '--o')  
plt.xlabel(r"$\lambda_1$ log-Scale", fontsize=22)
plt.ylabel("Mean 3-fold cross validation accuracy", fontsize=22)

plt.legend([round(0.1*(x+1),1) for x in range(9)], title="l1_ratio")

plt.xticks(fontsize=20)
plt.yticks(fontsize=20)

clf.best_params_

coefs = clf.best_estimator_.coef_

print(f"{np.mean(coefs == 0) * 100:.2f} % of features are zero!")

plt.figure(figsize=(20,10))
plt.hist(coefs.flatten())

plt.xticks(fontsize=20)
plt.yticks(fontsize=20)

fig, axes = plt.subplots(nrows=2, ncols=5, figsize=(20,10))

scale = np.abs(coefs).max()

for i, ax in enumerate(axes.flat):
  ax.set_axis_off()
  ax.set_title(f"{i}")
  img = coefs[i]
  img = img.reshape(28,28)
  im = ax.imshow(img, interpolation='nearest', vmin=-scale, vmax=scale, cmap=plt.cm.RdBu)

cbar = fig.colorbar(im, ax=axes.ravel().tolist())

"""### Ridge
For comparison, we also fit a logistic regression model with $\ell_2$-norm regularization.
"""

lr = LogisticRegression(tol=0.1)

param_grid = [
  {'C': [10**x for x in range(-5,5)], 'penalty': ['l2'], 'solver': ['saga']},
  ]

clf_ridge = GridSearchCV(lr, param_grid, cv=3, scoring='accuracy', n_jobs=2)

clf_ridge.fit(x_train, y_train)

clf_ridge.best_params_

print(f"LASSO: {lasso_accuracy.max() * 100}")
print(f"Ridge: {clf_ridge.best_score_ * 100}")
print(f"Elastic Net: {clf.best_score_ * 100}")

ridge_accuracy = clf_ridge.cv_results_['mean_test_score']

plt.figure(figsize=(20,10))
#flip because the hyperparameter in scikit learn C = 1/lambda
plt.plot(list(range(-5,5)), np.flip(ridge_accuracy), '--o')  
plt.xlabel(r"$\lambda_2$ log-Scale",fontsize=22)
plt.ylabel("Mean 3-fold cross validation accuracy",fontsize=22)

plt.xticks(fontsize=20)
plt.yticks(fontsize=20)

ridge_coefs = clf_ridge.best_estimator_.coef_

print(f"{np.mean(ridge_coefs == 0) * 100:.2f} % of features are zero!")

plt.figure(figsize=(20,10))
plt.hist(ridge_coefs.flatten())

plt.xticks(fontsize=20)
plt.yticks(fontsize=20)

fig, axes = plt.subplots(nrows=2, ncols=5, figsize=(20,10))

scale = np.abs(ridge_coefs).max()

for i, ax in enumerate(axes.flat):
  ax.set_axis_off()
  ax.set_title(f"{i}")
  img = ridge_coefs[i]
  img = img.reshape(28,28)
  im = ax.imshow(img, interpolation='nearest', vmin=-scale, vmax=scale, cmap=plt.cm.RdBu)

cbar = fig.colorbar(im, ax=axes.ravel().tolist())

"""# Ranking Pixels
We rank the pixels by each its corresponding coefficient for each digit.

### LASSO
"""

# most important pixels for each digit
lasso_coefs.argmax(axis=1)

n = 100

lasso_coefs_copy = lasso_coefs.copy()

arr = np.empty((0,784), float)
for row in lasso_coefs_copy:
  abs_row = abs(row)
  abs_row.sort()
  thresh = abs_row[-n]
  filter = abs(row) < thresh
  row[filter] = 0
  arr = np.append(arr, np.array([row]), axis=0)


fig, axes = plt.subplots(nrows=2, ncols=5, figsize=(20,10))

scale = np.abs(ridge_coefs).max()

for i, ax in enumerate(axes.flat):
  ax.set_axis_off()
  ax.set_title(f"{i}")
  img = arr[i]
  img = img.reshape(28,28)
  im = ax.imshow(img, interpolation='nearest', vmin=-scale, vmax=scale, cmap=plt.cm.RdBu)

cbar = fig.colorbar(im, ax=axes.ravel().tolist())

lasso_all_digit_rank = abs(lasso_coefs_copy).sum(axis=0)

lasso_all_digit_copy = lasso_all_digit_rank.copy()
lasso_all_digit_copy.sort()
thresh = lasso_all_digit_copy[-n]
filter = lasso_all_digit_rank < thresh
lasso_all_digit_rank[filter] = 0

scale = np.abs(lasso_all_digit_rank).max()

fig, ax = plt.subplots(figsize=(15, 15), ncols=1)

im = ax.imshow(lasso_all_digit_rank.reshape(28,28), interpolation='nearest',
               vmin=-scale, vmax=scale, cmap=plt.cm.RdBu)

fig.colorbar(im, ax=ax)

"""### Ridge"""

n = 100

ridge_coefs_copy = ridge_coefs.copy()

arr = np.empty((0,784), float)
for row in ridge_coefs_copy:
  abs_row = abs(row)
  abs_row.sort()
  thresh = abs_row[-n]
  filter = abs(row) < thresh
  row[filter] = 0
  arr = np.append(arr, np.array([row]), axis=0)


fig, axes = plt.subplots(nrows=2, ncols=5, figsize=(20,10))

scale = np.abs(ridge_coefs).max()

for i, ax in enumerate(axes.flat):
  ax.set_axis_off()
  ax.set_title(f"{i}")
  img = arr[i]
  img = img.reshape(28,28)
  im = ax.imshow(img, interpolation='nearest', vmin=-scale, vmax=scale, cmap=plt.cm.RdBu)

cbar = fig.colorbar(im, ax=axes.ravel().tolist())

ridge_all_digit_rank = abs(ridge_coefs_copy).sum(axis=0)

ridge_all_digit_copy = ridge_all_digit_rank.copy()
ridge_all_digit_copy.sort()
thresh = ridge_all_digit_copy[-n]
filter = ridge_all_digit_rank < thresh
ridge_all_digit_rank[filter] = 0

scale = np.abs(ridge_all_digit_rank).max()

fig, ax = plt.subplots(figsize=(15, 15), ncols=1)

im = ax.imshow(ridge_all_digit_rank.reshape(28,28), interpolation='nearest',
               vmin=-scale, vmax=scale, cmap=plt.cm.RdBu)

fig.colorbar(im, ax=ax)

"""## Elastic Net"""

n = 100

en_coefs_copy = clf.best_estimator_.coef_.copy()

arr = np.empty((0,784), float)
for row in en_coefs_copy:
  abs_row = abs(row)
  abs_row.sort()
  thresh = abs_row[-n]
  filter = abs(row) < thresh
  row[filter] = 0
  arr = np.append(arr, np.array([row]), axis=0)


fig, axes = plt.subplots(nrows=2, ncols=5, figsize=(20,10))

scale = np.abs(ridge_coefs).max()

for i, ax in enumerate(axes.flat):
  ax.set_axis_off()
  ax.set_title(f"{i}")
  img = arr[i]
  img = img.reshape(28,28)
  im = ax.imshow(img, interpolation='nearest', vmin=-scale, vmax=scale, cmap=plt.cm.RdBu)

cbar = fig.colorbar(im, ax=axes.ravel().tolist())

en_all_digit_rank = abs(en_coefs_copy).sum(axis=0)

en_all_digit_copy = en_all_digit_rank.copy()
en_all_digit_copy.sort()
thresh = en_all_digit_copy[-n]
filter = en_all_digit_rank < thresh
en_all_digit_rank[filter] = 0

scale = np.abs(en_all_digit_rank).max()

fig, ax = plt.subplots(figsize=(15, 15), ncols=1)

im = ax.imshow(en_all_digit_rank.reshape(28,28), interpolation='nearest',
               vmin=-scale, vmax=scale, cmap=plt.cm.RdBu)

fig.colorbar(im, ax=ax)

"""# Test set

### LASSO
"""

yhat =clf_lasso.predict(x_test)

print(f"Test accuracy {accuracy_score(y_test, yhat)*100}%")
confusion_matrix(y_test, yhat)

print(classification_report(y_test, yhat, target_names=['0', '1', '2', '3', '4', '5', '6', '7', '8', '9'], digits=3))

fig, ax = plt.subplots(figsize=(15, 15))


plot_confusion_matrix(clf_lasso, x_test, y_test, ax=ax, values_format='.4g')

"""### Ridge"""

confusion_matrix(y_test, clf_ridge.predict(x_test))

print(f"Test accuracy {accuracy_score(y_test, clf_ridge.predict(x_test))*100}%")

print(classification_report(y_test,  clf_ridge.predict(x_test), target_names=['0', '1', '2', '3', '4', '5', '6', '7', '8', '9'], digits=3))

fig, ax = plt.subplots(figsize=(15, 15))

plot_confusion_matrix(clf_ridge, x_test, y_test, ax=ax, values_format = '.4g')

"""### Elastic Net"""

confusion_matrix(y_test, clf.best_estimator_.predict(x_test))

print(f"Test accuracy {accuracy_score(y_test, clf.best_estimator_.predict(x_test))*100}%")

print(classification_report(y_test, clf.best_estimator_.predict(x_test), target_names=['0', '1', '2', '3', '4', '5', '6', '7', '8', '9'], digits=3))

fig, ax = plt.subplots(figsize=(15, 15))

plot_confusion_matrix(clf.best_estimator_, x_test, y_test, ax=ax, values_format = '.4g')

"""## Sparse Test Evaluation
Using the top 100 pixels from each model we evaluate on the test set.

### LASSO
"""

lasso_sparse_subset = lasso_all_digit_rank.nonzero()

lasso_sparse_subset

# subset training and test sets to top 100 pixels
x_train_sparse_lasso = np.transpose(np.transpose(x_train)[lasso_sparse_subset])
x_test_sparse_lasso = np.transpose(np.transpose(x_test)[lasso_sparse_subset])

clf_lasso_sparse = LogisticRegression(C= 100, penalty='l1', solver='saga', tol=0.1)
clf_lasso_sparse.fit(x_train_sparse_lasso, y_train)

yhat =clf_lasso_sparse.predict(x_test_sparse_lasso)
print(f"Test accuracy {accuracy_score(y_test, yhat)*100}%")
print(classification_report(
    y_test, yhat,
    target_names=['0', '1', '2', '3', '4', '5', '6', '7', '8', '9'],
    digits=3)
)

fig, ax = plt.subplots(figsize=(15, 15))

plot_confusion_matrix(clf_lasso_sparse, x_test_sparse_lasso, y_test, ax=ax, values_format = '.4g')

"""### Ridge"""

ridge_sparse_subset = ridge_all_digit_rank.nonzero()

# subset training and test sets to top 100 pixels
x_train_sparse_ridge = np.transpose(np.transpose(x_train)[ridge_sparse_subset])
x_test_sparse_ridge = np.transpose(np.transpose(x_test)[ridge_sparse_subset])

clf_ridge_sparse = LogisticRegression(C= 10000, penalty='l2', solver='saga', tol=0.1)
clf_ridge_sparse.fit(x_train_sparse_ridge, y_train)

yhat =clf_ridge_sparse.predict(x_test_sparse_ridge)
print(f"Test accuracy {accuracy_score(y_test, yhat)*100}%")
print(classification_report(
    y_test, yhat,
    target_names=['0', '1', '2', '3', '4', '5', '6', '7', '8', '9'],
    digits=3)
)

fig, ax = plt.subplots(figsize=(15, 15))

plot_confusion_matrix(clf_ridge_sparse, x_test_sparse_ridge, y_test, ax=ax, values_format = '.4g')

"""### Elastic Net"""

en_sparse_subset = en_all_digit_rank.nonzero()

# subset training and test sets to top 100 pixels
x_train_sparse_en = np.transpose(np.transpose(x_train)[en_sparse_subset])
x_test_sparse_en = np.transpose(np.transpose(x_test)[en_sparse_subset])

clf.best_params_

clf_en_sparse = LogisticRegression(C= 100, penalty='elasticnet', l1_ratio=0.8, solver='saga', tol=0.1)
clf_en_sparse.fit(x_train_sparse_en, y_train)

yhat =clf_en_sparse.predict(x_test_sparse_en)
print(f"Test accuracy {accuracy_score(y_test, yhat)*100}%")
print(classification_report(
    y_test, yhat,
    target_names=['0', '1', '2', '3', '4', '5', '6', '7', '8', '9'],
    digits=3)
)

fig, ax = plt.subplots(figsize=(15, 15))

plot_confusion_matrix(clf_en_sparse, x_test_sparse_en, y_test, ax=ax, values_format = '.4g')

